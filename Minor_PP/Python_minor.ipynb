{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5897ee05-36fe-452e-ad60-1a114de9c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4425ef-8892-4471-bcb0-9ea35b69420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SprintBacklogAnalyzer:\n",
    "    def __init__(self, data_path=None, df=None):\n",
    "        \"\"\"Initialize the analyzer with either a path to CSV or a dataframe\"\"\"\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        elif data_path:\n",
    "            self.df = pd.read_csv(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either data_path or df must be provided\")\n",
    "        \n",
    "        self.models = {}\n",
    "        self.preprocessors = {}\n",
    "        self.feature_importances = {}\n",
    "        self.results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4537b8cb-e709-4186-9ea6-b7b6786be2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self):\n",
    "        \"\"\"Clean and preprocess the data\"\"\"\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        df['DEADLINE (MINUTES)'] = df['DEADLINE (MINUTES)'].fillna(df['DURATION (MINUTES)'].median())\n",
    "        \n",
    "        # Feature extraction from ticket titles\n",
    "        df['TASK_TYPE'] = df['TICKET TITLE'].apply(lambda x: x.split(' - ')[0] if ' - ' in x else x.split(' ')[0])\n",
    "        \n",
    "        # Create additional features\n",
    "        df['DEADLINE_MET'] = df['DURATION (MINUTES)'] <= df['DEADLINE (MINUTES)']\n",
    "        df['EFFICIENCY_RATIO'] = df['STORY POINT'] / df['DURATION (MINUTES)']\n",
    "        \n",
    "        # Categorical variables mapping for better interpretability\n",
    "        priority_map = {'Highest': 4, 'High': 3, 'Medium': 2, 'Low': 1}\n",
    "        df['PRIORITY_NUM'] = df['PRIORITY'].map(priority_map)\n",
    "        \n",
    "        # Extract sprint number\n",
    "        df['SPRINT_NUM'] = df['SPRINT NAME'].str.extract(r'(\\d+)').astype(int)\n",
    "        \n",
    "        self.processed_df = df\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e86a0d1-381a-44d5-ab77-c98cb8032305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(self, train_data, test_data=None):\n",
    "        \"\"\"Extract features from ticket titles using TF-IDF\"\"\"\n",
    "        tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "        \n",
    "        if test_data is not None:\n",
    "            # For training and testing separately\n",
    "            train_text_features = tfidf.fit_transform(train_data['TICKET TITLE'])\n",
    "            test_text_features = tfidf.transform(test_data['TICKET TITLE'])\n",
    "            self.tfidf_vectorizer = tfidf\n",
    "            return train_text_features, test_text_features\n",
    "        else:\n",
    "            # For single transformation\n",
    "            text_features = tfidf.fit_transform(train_data['TICKET TITLE'])\n",
    "            self.tfidf_vectorizer = tfidf\n",
    "            return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ad9c46-9058-4c22-8f16-5aae1240d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_duration_prediction_model(self):\n",
    "        \"\"\"Build a model to predict task duration\"\"\"\n",
    "        df = self.processed_df.copy()\n",
    "        \n",
    "        # Define features and target\n",
    "        X = df[['PROJECT NAME (ANONYMISED)', 'SPRINT_NUM', 'TASK_TYPE', 'STORY POINT', \n",
    "                'PRIORITY', 'DEADLINE (MINUTES)']]\n",
    "        y = df['DURATION (MINUTES)']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Get text features\n",
    "        train_text, test_text = self.extract_text_features(df.loc[X_train.index], df.loc[X_test.index])\n",
    "        \n",
    "        # Define preprocessing for numerical features\n",
    "        numerical_features = ['STORY POINT', 'DEADLINE (MINUTES)', 'SPRINT_NUM']\n",
    "        numerical_transformer = StandardScaler()\n",
    "        \n",
    "        # Define preprocessing for categorical features\n",
    "        categorical_features = ['PROJECT NAME (ANONYMISED)', 'TASK_TYPE', 'PRIORITY']\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "        \n",
    "        # Combine preprocessing steps\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        # Prepare train and test features\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Add text features\n",
    "        if isinstance(X_train_processed, np.ndarray):\n",
    "            X_train_processed = np.hstack((X_train_processed, train_text.toarray()))\n",
    "            X_test_processed = np.hstack((X_test_processed, test_text.toarray()))\n",
    "        else:\n",
    "            # If sparse matrix\n",
    "            from scipy.sparse import hstack\n",
    "            X_train_processed = hstack([X_train_processed, train_text])\n",
    "            X_test_processed = hstack([X_test_processed, test_text])\n",
    "        \n",
    "        # Train models\n",
    "        models = {\n",
    "            'linear': LinearRegression(),\n",
    "            'rf': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'gbm': GradientBoostingRegressor(random_state=42)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            # Train the model\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            \n",
    "            # Evaluate\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'r2': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"Model: {name}\")\n",
    "            print(f\"MSE: {mse:.2f}\")\n",
    "            print(f\"RMSE: {rmse:.2f}\")\n",
    "            print(f\"MAE: {mae:.2f}\")\n",
    "            print(f\"R²: {r2:.2f}\")\n",
    "            print(\"-------------------\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_model_name = max(results, key=lambda x: results[x]['r2'])\n",
    "        print(f\"Best model: {best_model_name} with R² of {results[best_model_name]['r2']:.2f}\")\n",
    "        \n",
    "        # Save best model and preprocessor\n",
    "        self.models['duration'] = results[best_model_name]['model']\n",
    "        self.preprocessors['duration'] = preprocessor\n",
    "        self.results['duration'] = results\n",
    "        \n",
    "        # Feature importance for random forest\n",
    "        if 'rf' in results:\n",
    "            # Get feature names\n",
    "            feature_names = []\n",
    "            # Get numerical feature names\n",
    "            for name in numerical_features:\n",
    "                feature_names.append(name)\n",
    "            \n",
    "            # Get one-hot encoded feature names\n",
    "            for i, name in enumerate(categorical_features):\n",
    "                categories = preprocessor.transformers_[1][1].categories_[i]\n",
    "                for category in categories:\n",
    "                    feature_names.append(f\"{name}_{category}\")\n",
    "            \n",
    "            # Add text feature names (simplified)\n",
    "            for i in range(train_text.shape[1]):\n",
    "                feature_names.append(f\"text_feature_{i}\")\n",
    "            \n",
    "            # Get feature importances\n",
    "            importances = results['rf']['model'].feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Limit to top 20 features\n",
    "            top_n = min(20, len(feature_names))\n",
    "            \n",
    "            self.feature_importances['duration'] = {\n",
    "                'names': [feature_names[i] for i in indices[:top_n]],\n",
    "                'scores': [importances[i] for i in indices[:top_n]]\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf3678c-eacc-4629-abd9-2dd4d4fbea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deadline_classification_model(self):\n",
    "        \"\"\"Build a model to predict if a task will meet its deadline\"\"\"\n",
    "        df = self.processed_df.copy()\n",
    "        \n",
    "        # Define features and target\n",
    "        X = df[['PROJECT NAME (ANONYMISED)', 'SPRINT_NUM', 'TASK_TYPE', \n",
    "                'STORY POINT', 'PRIORITY', 'DEADLINE (MINUTES)']]\n",
    "        y = df['DEADLINE_MET']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Get text features\n",
    "        train_text, test_text = self.extract_text_features(df.loc[X_train.index], df.loc[X_test.index])\n",
    "        \n",
    "        # Define preprocessing for numerical features\n",
    "        numerical_features = ['STORY POINT', 'DEADLINE (MINUTES)', 'SPRINT_NUM']\n",
    "        numerical_transformer = StandardScaler()\n",
    "        \n",
    "        # Define preprocessing for categorical features\n",
    "        categorical_features = ['PROJECT NAME (ANONYMISED)', 'TASK_TYPE', 'PRIORITY']\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "        \n",
    "        # Combine preprocessing steps\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        # Prepare train and test features\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Add text features\n",
    "        if isinstance(X_train_processed, np.ndarray):\n",
    "            X_train_processed = np.hstack((X_train_processed, train_text.toarray()))\n",
    "            X_test_processed = np.hstack((X_test_processed, test_text.toarray()))\n",
    "        else:\n",
    "            # If sparse matrix\n",
    "            from scipy.sparse import hstack\n",
    "            X_train_processed = hstack([X_train_processed, train_text])\n",
    "            X_test_processed = hstack([X_test_processed, test_text])\n",
    "        \n",
    "        # Train logistic regression model\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        results = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'report': report\n",
    "        }\n",
    "        \n",
    "        print(f\"Deadline Classification Model\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save model and preprocessor\n",
    "        self.models['deadline'] = model\n",
    "        self.preprocessors['deadline'] = preprocessor\n",
    "        self.results['deadline'] = results\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645f9c48-b4f6-4fab-a938-da0e57c82b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_task_duration(self, project, sprint, title, story_point, priority, deadline):\n",
    "        \"\"\"Predict the duration of a new task\"\"\"\n",
    "        if 'duration' not in self.models:\n",
    "            raise ValueError(\"Duration prediction model not trained yet. Call build_duration_prediction_model() first.\")\n",
    "        \n",
    "        # Create a dataframe with the new task\n",
    "        task = pd.DataFrame({\n",
    "            'PROJECT NAME (ANONYMISED)': [project],\n",
    "            'SPRINT_NUM': [int(sprint.replace('Sprint ', ''))],\n",
    "            'TICKET TITLE': [title],\n",
    "            'TASK_TYPE': [title.split(' - ')[0] if ' - ' in title else title.split(' ')[0]],\n",
    "            'STORY POINT': [story_point],\n",
    "            'PRIORITY': [priority],\n",
    "            'DEADLINE (MINUTES)': [deadline]\n",
    "        })\n",
    "        \n",
    "        # Preprocess the features\n",
    "        preprocessor = self.preprocessors['duration']\n",
    "        X = task[['PROJECT NAME (ANONYMISED)', 'SPRINT_NUM', 'TASK_TYPE', 'STORY POINT', \n",
    "                 'PRIORITY', 'DEADLINE (MINUTES)']]\n",
    "        \n",
    "        # Transform numerical and categorical features\n",
    "        X_processed = preprocessor.transform(X)\n",
    "        \n",
    "        # Transform text features\n",
    "        text_processed = self.tfidf_vectorizer.transform(task['TICKET TITLE'])\n",
    "        \n",
    "        # Combine features\n",
    "        if isinstance(X_processed, np.ndarray):\n",
    "            X_final = np.hstack((X_processed, text_processed.toarray()))\n",
    "        else:\n",
    "            # If sparse matrix\n",
    "            from scipy.sparse import hstack\n",
    "            X_final = hstack([X_processed, text_processed])\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_duration = self.models['duration'].predict(X_final)[0]\n",
    "        \n",
    "        return predicted_duration\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9626ffdf-04d1-486b-ab43-a13c6e2d4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_deadline_met(self, project, sprint, title, story_point, priority, deadline):\n",
    "        \"\"\"Predict if a task will meet its deadline\"\"\"\n",
    "        if 'deadline' not in self.models:\n",
    "            raise ValueError(\"Deadline classification model not trained yet. Call build_deadline_classification_model() first.\")\n",
    "        \n",
    "        # Create a dataframe with the new task\n",
    "        task = pd.DataFrame({\n",
    "            'PROJECT NAME (ANONYMISED)': [project],\n",
    "            'SPRINT_NUM': [int(sprint.replace('Sprint ', ''))],\n",
    "            'TICKET TITLE': [title],\n",
    "            'TASK_TYPE': [title.split(' - ')[0] if ' - ' in title else title.split(' ')[0]],\n",
    "            'STORY POINT': [story_point],\n",
    "            'PRIORITY': [priority],\n",
    "            'DEADLINE (MINUTES)': [deadline]\n",
    "        })\n",
    "        \n",
    "        # Preprocess the features\n",
    "        preprocessor = self.preprocessors['deadline']\n",
    "        X = task[['PROJECT NAME (ANONYMISED)', 'SPRINT_NUM', 'TASK_TYPE', 'STORY POINT', \n",
    "                 'PRIORITY', 'DEADLINE (MINUTES)']]\n",
    "        \n",
    "        # Transform numerical and categorical features\n",
    "        X_processed = preprocessor.transform(X)\n",
    "        \n",
    "        # Transform text features\n",
    "        text_processed = self.tfidf_vectorizer.transform(task['TICKET TITLE'])\n",
    "        \n",
    "        # Combine features\n",
    "        if isinstance(X_processed, np.ndarray):\n",
    "            X_final = np.hstack((X_processed, text_processed.toarray()))\n",
    "        else:\n",
    "            # If sparse matrix\n",
    "            from scipy.sparse import hstack\n",
    "            X_final = hstack([X_processed, text_processed])\n",
    "        \n",
    "        # Make prediction\n",
    "        deadline_met = self.models['deadline'].predict(X_final)[0]\n",
    "        deadline_prob = self.models['deadline'].predict_proba(X_final)[0][1]  # Probability of meeting deadline\n",
    "        \n",
    "        return deadline_met, deadline_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becef235-61fb-4387-9c0d-7acdcc4fa9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_duration_distribution(self):\n",
    "        \"\"\"Visualize the distribution of task durations\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.histplot(self.processed_df['DURATION (MINUTES)'], bins=30, kde=True, ax=ax)\n",
    "        ax.set_title('Distribution of Task Durations')\n",
    "        ax.set_xlabel('Duration (minutes)')\n",
    "        ax.set_ylabel('Count')\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32bd0a6-89c8-4d53-9850-e38ff1e9e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_story_points_vs_duration(self):\n",
    "        \"\"\"Visualize the relationship between story points and duration\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxplot(x='STORY POINT', y='DURATION (MINUTES)', data=self.processed_df, ax=ax)\n",
    "        ax.set_title('Task Duration by Story Points')\n",
    "        ax.set_xlabel('Story Points')\n",
    "        ax.set_ylabel('Duration (minutes)')\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bd9859-69d8-497d-9980-9c2e1c080636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_priority_vs_deadline_met(self):\n",
    "        \"\"\"Visualize the relationship between priority and meeting deadlines\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        priority_deadline = self.processed_df.groupby('PRIORITY')['DEADLINE_MET'].mean().reset_index()\n",
    "        priority_deadline = priority_deadline.sort_values(by='PRIORITY_NUM', key=lambda x: x.map({\n",
    "            'Highest': 4, 'High': 3, 'Medium': 2, 'Low': 1\n",
    "        }))\n",
    "        \n",
    "        sns.barplot(x='PRIORITY', y='DEADLINE_MET', data=priority_deadline, ax=ax)\n",
    "        ax.set_title('Proportion of Tasks Meeting Deadline by Priority')\n",
    "        ax.set_xlabel('Priority')\n",
    "        ax.set_ylabel('Proportion Meeting Deadline')\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46327844-196f-45de-b9ff-0d1654fc47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importance(self):\n",
    "        \"\"\"Visualize feature importances for the duration prediction model\"\"\"\n",
    "        if 'duration' not in self.feature_importances:\n",
    "            raise ValueError(\"Feature importances not available. Train Random Forest model first.\")\n",
    "        \n",
    "        importances = self.feature_importances['duration']\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        y_pos = np.arange(len(importances['names']))\n",
    "        ax.barh(y_pos, importances['scores'], align='center')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(importances['names'])\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title('Feature Importance for Duration Prediction')\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "099af122-a2f1-40ba-8239-b4a0de85ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(self, directory='models'):\n",
    "        \"\"\"Save trained models and preprocessors to disk\"\"\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            joblib.dump(model, os.path.join(directory, f\"{model_name}_model.pkl\"))\n",
    "        \n",
    "        for preprocessor_name, preprocessor in self.preprocessors.items():\n",
    "            joblib.dump(preprocessor, os.path.join(directory, f\"{preprocessor_name}_preprocessor.pkl\"))\n",
    "        \n",
    "        # Save TF-IDF vectorizer\n",
    "        if hasattr(self, 'tfidf_vectorizer'):\n",
    "            joblib.dump(self.tfidf_vectorizer, os.path.join(directory, \"tfidf_vectorizer.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe3356d-836b-45ea-98a6-6acab0721887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(self, directory='models'):\n",
    "        \"\"\"Load trained models and preprocessors from disk\"\"\"\n",
    "        for model_file in os.listdir(directory):\n",
    "            if model_file.endswith('_model.pkl'):\n",
    "                model_name = model_file.split('_')[0]\n",
    "                self.models[model_name] = joblib.load(os.path.join(directory, model_file))\n",
    "            \n",
    "            if model_file.endswith('_preprocessor.pkl'):\n",
    "                preprocessor_name = model_file.split('_')[0]\n",
    "                self.preprocessors[preprocessor_name] = joblib.load(os.path.join(directory, model_file))\n",
    "            \n",
    "            if model_file == \"tfidf_vectorizer.pkl\":\n",
    "                self.tfidf_vectorizer = joblib.load(os.path.join(directory, model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e17fd4b9-66b1-40bd-99b2-84e96ecf6496",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Sample Agile Data for KIIT -v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Initialize analyzer with data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mSprintBacklogAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSample Agile Data for KIIT -v2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     analyzer\u001b[38;5;241m.\u001b[39mpreprocess_data()\n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36mSprintBacklogAnalyzer.__init__\u001b[1;34m(self, data_path, df)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data_path:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither data_path or df must be provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Sample Agile Data for KIIT -v2.csv'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer with data\n",
    "    analyzer = SprintBacklogAnalyzer(data_path=\"Sample Agile Data for KIIT -v2.csv\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    analyzer.preprocess_data()\n",
    "    \n",
    "    # Build and train models\n",
    "    analyzer.build_duration_prediction_model()\n",
    "    analyzer.build_deadline_classification_model()\n",
    "    \n",
    "    # Save models for later use\n",
    "    analyzer.save_models()\n",
    "    \n",
    "    # Make some predictions\n",
    "    predicted_duration = analyzer.predict_task_duration(\n",
    "        project=\"ACME\",\n",
    "        sprint=\"Sprint 4\",\n",
    "        title=\"Frontend - Implement new login form\",\n",
    "        story_point=2,\n",
    "        priority=\"High\",\n",
    "        deadline=120\n",
    "    )\n",
    "    \n",
    "    deadline_met, deadline_prob = analyzer.predict_deadline_met(\n",
    "        project=\"ACME\",\n",
    "        sprint=\"Sprint 4\",\n",
    "        title=\"Frontend - Implement new login form\",\n",
    "        story_point=2,\n",
    "        priority=\"High\",\n",
    "        deadline=120\n",
    "    )\n",
    "    \n",
    "    print(f\"Predicted duration: {predicted_duration:.2f} minutes\")\n",
    "    print(f\"Will meet deadline: {deadline_met} (Probability: {deadline_prob:.2f})\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    analyzer.visualize_duration_distribution()\n",
    "    analyzer.visualize_story_points_vs_duration()\n",
    "    analyzer.visualize_priority_vs_deadline_met()\n",
    "    \n",
    "    try:\n",
    "        analyzer.visualize_feature_importance()\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
